title: 大数据分析初探
date: 2016-12-11 10:35:53
categories:
- 技术

tags: 
- 大数据分析 
---

接触数据分析不久，有一些感性的粗浅认识。无论是传统的数据分析还是当下流行的大数据分析都试图在数据之下挖掘潜在的价值。对于不同的场景与业务而言，数据分析的目标大相径庭：可能是对某个业务进行评估（过去），也可能是识别某些有风险的操作（现在），亦或是对行为进行预测（未来），当然更多情况下是上述目标的组合。但是无论如何这个过程必然涉及到三个基本的问题：输入有哪些，处理方式如何，最终结果输出的形式是什么。下文首先简单分析一下传统数据分析与大数据分析的区别，然后聊一聊数据处理的三个基本问题，着重介绍数据处理的方式（以二分模型为例）。

## 什么是数据分析

数据分析是人们期望在不确定的世界中寻找那么一点规律与结论，来指导决策的一种方式。因此，对于传统数据分析和大数据分析而言，它们的目标相同，基本思想相似。只是在实践中的他们的手段与形式不同。那么，为什么近年来大数据分析成为热门的话题呢？可以说这是时代发展的必然产物。互联网的迅速发展使得大量人和物连接到了一起，他们随时随刻都在产生大量数据。网民的每一次点击，本质上是个体的一种决策行为；物联网设备收集的真实世界的信息，正把社会数字化。这些是大数据分析发展的前提条件。同时，软硬件技术的发展使得大量数据集的处理成为可能，人们以对数据进行全量分析从而得到更具真实的信息。再者，个性化需求场景的不断涌现，需要大数据技术实时分析用户需求。而且，大数据分析提供了很多新的工具，它使得含有海量特征的模型成为可能，赋予机器学习技术更多的可能。这些都是传统的数据分析技术不具备的能力。

## 数据之源

大数据分析的对象分为以下几种

* 数据库中的**结构化数据**。典型的如用户的账户信息，交易记录，访问记录。
* 文件中大量的**半结构化数据**。如日志文件，点击数据，搜索内容，行为序列，气候记录等。
* 互联网中海量**非结构化数据**。如社交网络上用户的评论信息，舆论新闻等。

除了传统关系型数据库中这些数据，大数据分析的对象往往有一个特点，包含大量的（暂时）无用内容。往往需要我们进行过滤清洗（ETL），提取出分析需要的有价值的内容。

不同的数据质量与价值不同，如包含用户真实社交信息的数据的价值可能远远大于网页爬取的用户数据；包含交易信息的数据可能比浏览数据更加有价值；拥有场景信息的数据比孤立的数据更珍贵。因而，拥有这些数据的公司自然身价不菲。但是不能一概而论，如何合理使用数据，挖掘价值也是关键的一步。

在学习实践时，我们可以使用一些公开的数据集，或者公司内部的数据，甚至自己编写爬虫从网页中抓取数据（如爬取所有知乎用户的列表等）。它们的数据量可能从几GB到几个TB，PB不等，如何存储，处理这些数据是大数据分析的核心内容。

## 分析之路

### 数据存储

#### 分布式文件系统

#### 文件存储格式

* 列式存储
* 行式存储

### 分布式计算

#### 资源分配

#### 资源调度

#### 磁盘计算-MR

#### 内存计算-Spark

### 流式计算

#### 小批量流式

#### 实时流式

### 容错与恢复

### 策略

#### 统计指标

#### 分布

#### 模型

## 结果之殇

数据分析结果的表现形式，可能是一句话，一个PPT，一张报表，一份分析报告或者是一个模型。然而结果可信吗？如何评估我们的分析结果？对于常见的二分模型，我们使用正确率与召回率这两个指标来评估结果。












<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"limuzhi.com","root":"/","scheme":"Mist","version":"7.7.1","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="参考资料：  书籍：《Spark快速大数据分析》 官方文档： 编程 快速入门&amp;amp;&amp;amp;提交作业文档&amp;amp;&amp;amp;基本术语：基本入门须知 编程指南：介绍了Spark的基本API（RDD等）必看资料 Spark SQL编程文档：介绍了Spark的DataFrame与DataSet API，也是必读资料。   配置 spark配置文档：spark的基本配置，如spark-defaults">
<meta name="keywords" content="大数据分析,Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark入门--配置与API编程">
<meta property="og:url" content="http://limuzhi.com/2017/01/29/Spark入门--配置与API编程/index.html">
<meta property="og:site_name" content="Night Piece">
<meta property="og:description" content="参考资料：  书籍：《Spark快速大数据分析》 官方文档： 编程 快速入门&amp;amp;&amp;amp;提交作业文档&amp;amp;&amp;amp;基本术语：基本入门须知 编程指南：介绍了Spark的基本API（RDD等）必看资料 Spark SQL编程文档：介绍了Spark的DataFrame与DataSet API，也是必读资料。   配置 spark配置文档：spark的基本配置，如spark-defaults">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://spark.apache.org/docs/latest/img/cluster-overview.png">
<meta property="og:updated_time" content="2020-02-25T09:44:15.475Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark入门--配置与API编程">
<meta name="twitter:description" content="参考资料：  书籍：《Spark快速大数据分析》 官方文档： 编程 快速入门&amp;amp;&amp;amp;提交作业文档&amp;amp;&amp;amp;基本术语：基本入门须知 编程指南：介绍了Spark的基本API（RDD等）必看资料 Spark SQL编程文档：介绍了Spark的DataFrame与DataSet API，也是必读资料。   配置 spark配置文档：spark的基本配置，如spark-defaults">
<meta name="twitter:image" content="http://spark.apache.org/docs/latest/img/cluster-overview.png">

<link rel="canonical" href="http://limuzhi.com/2017/01/29/Spark入门--配置与API编程/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Spark入门--配置与API编程 | Night Piece</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Night Piece</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">white && black</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tools">

    <a href="/tools/" rel="section"><i class="fa fa-fw fa-wrench"></i>利器</a>

  </li>
        <li class="menu-item menu-item-read">

    <a href="/read/" rel="section"><i class="fa fa-fw fa-book"></i>阅读</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://limuzhi.com/2017/01/29/Spark入门--配置与API编程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="limuzhi">
      <meta itemprop="description" content="something about tech, android etc...">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Night Piece">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark入门--配置与API编程
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-01-29 16:28:53" itemprop="dateCreated datePublished" datetime="2017-01-29T16:28:53+08:00">2017-01-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-25 17:44:15" itemprop="dateModified" datetime="2020-02-25T17:44:15+08:00">2020-02-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2017/01/29/Spark入门--配置与API编程/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2017/01/29/Spark入门--配置与API编程/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>参考资料：</p>
<ul>
<li>书籍：《Spark快速大数据分析》</li>
<li>官方文档：<ul>
<li>编程<ul>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="noopener">快速入门</a>&amp;&amp;<a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">提交作业文档</a>&amp;&amp;<a href="http://spark.apache.org/docs/latest/cluster-overview.html#glossary" target="_blank" rel="noopener">基本术语</a>：基本入门须知</li>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="noopener">编程指南</a>：介绍了Spark的基本API（RDD等）必看资料</li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">Spark SQL编程文档</a>：介绍了Spark的DataFrame与DataSet API，也是必读资料。</li>
</ul>
</li>
<li>配置<ul>
<li><a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">spark配置文档</a>：spark的基本配置，如<code>spark-defaults.conf</code>文件</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">在yarn上运行spark</a>：在yarn上运行Spark的一些配置，如外部shuffle</li>
</ul>
</li>
<li>原理<ul>
<li><a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">Spark的集群模式简介</a>：集群cluster模式下Spark的运行组成。</li>
<li><a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">job调度，资源分配</a>：介绍了调度的策略（应用间与应用内，动态资源分配）</li>
</ul>
</li>
<li><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">spark调优指南</a>：从序列化，内存等角度讨论了Spark的性能优化。</li>
<li><a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">监控和一些工具</a>：Spark监控相关的设置，API接口，一些测量Metric的方法。</li>
</ul>
</li>
</ul>
<p>知识图谱：</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spark的官方介绍是：Apache Spark is a <strong>fast</strong> and <strong>general-purpose</strong> cluster computing system。这体现了Spark两个关键的特点：快速和通用。快速是指Spark与Hadoop相比具有可以在内存内快速处理数据的特点；通用是指Spark是一个大一统的软件栈，能使用一套API完成以前需要多个平台才能完成的任务，如：交互查询（Spark Shell），离线批量数据处理（RDD、Spark SQL），在线的流式处理（Spark Streaming），以及在大型数据集上运行迭代算法（ML任务）。</p>
<p>从工程角度看Spark具有下面一下特点：</p>
<ul>
<li>支持多种语言编程，Scala（优先推荐，也是编写Spark的语言），Python，Java（不推荐，编写代码复杂）。</li>
<li>具有Local模式，Local[n]（类似伪分布式）模式，集群模式（StandAlone/Yarn/Mesos三种方式）。</li>
<li>可以读取所有支持HDFS接口的文件系统（本地文件，HDFS上的文件，Hive，Hbase，亚马逊S3）</li>
<li>底层文件系统推荐HDFS，同时推荐集群模式运行在Hadoop Yarn之上（Why？？）</li>
</ul>
<p>Spark能完成的一些任务：</p>
<ul>
<li>交互式分析与查询：用于快速的数据分析或者调试程序。</li>
<li>Spark应用：以jar包的形式提交的Spark任务，可以执行离线（批量）或者在线任务（流式）。</li>
</ul>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><p>SparkContext：我们程序与Spark打交道的入口，代表对集群的一个连接。无论如何使用Spark，第一步永远都是初始化SparkContext(理解为运行的上下文！)，之后可以使用sc进行一些列操作，如配置集群，读取文件等。</p>
</li>
<li><p>Driver（驱动程序） &amp;&amp; Executor（执行器）：驱动程序简单理解成Master，全局唯一，实际运行我们编写的程序（即<strong>main函数入口</strong>），<strong>最终解析成一个个Task</strong>发送给下面的工作节点执行具体的操作。执行器理解成Slave，在系统中存在多个，并且对于一个Application而言，Executor是存在与它完整的生命周期中的（常驻的）。因此，依据这个基本原理，要注意：<strong>我们编写程序有一部分是运行在Driver中，一部分是发送到Executor中的执行</strong>。<u>常见的错误</u>是在foreach的回调函数中修改外部变量。本质上是修改的每个Executor的副本。</p>
</li>
<li><p>Client Mode 与 Cluster Mode：两者最大的区别是Driver程序的位置。如果Driver在本地则是Client Mode（典型地，交互式应用Spark Shell）。如果Driver在集群中的某个Node运行，则是Cluster模式。（对于Yarn模式的Client，Driver在Application Master中）</p>
<p><img src="http://spark.apache.org/docs/latest/img/cluster-overview.png" alt="Cluster Mode(Cluster Manager is standalone/Yarn/Mesos)"></p>
</li>
<li><p>Application &amp;&amp; Job &amp;&amp; Stage &amp;&amp; Task：这几个概念十分容易混淆，从左到右由大变小。</p>
<ul>
<li>Application：Spark上运行的用户程序，包括Driver和Executor。一个Application包含多个Job。</li>
<li>Job：作业，由<strong>Spark的驱动程序中的Action操作触发</strong>。它代表了一系列操作。</li>
<li>Stage：把一个Job按照某种规则拆分成n块，每一块是一个Stage。（规则简单理解成是否有网络数据传输Shuffle）</li>
<li>Task：实际执行的最小单元，任务的最小单元由Executor具体执行。</li>
</ul>
</li>
<li><p>一些常识概念：</p>
<ul>
<li>失败重试：某个节点的Task失败会自动重试。</li>
<li>长尾任务：某些节点很慢的情况下，会自动启动其他节点运行，哪个节点先完成则使用谁的结果。</li>
<li>就近执行：尽量减少数据网络传输，尽量在本地、一个机架内完成数据处理。</li>
</ul>
</li>
</ul>
<h2 id="开发环境配置"><a href="#开发环境配置" class="headerlink" title="开发环境配置"></a>开发环境配置</h2><p>详细参考官方文档，这里主要介绍Yarn模式</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit提交应用文档</a>：应用提交命令的用法</li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark配置文档</a>：配置大全</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.htm" target="_blank" rel="noopener">Yarn配置文档</a>：一些仅仅在Yarn下有效的配置</li>
<li><a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">Moniror配置文档</a>：配置History服务器</li>
</ul>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置上面提到的几种模式，这里重点提一下可以设置的地方。参考<a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">配置文档</a>。一个技巧，可以在<code>spark-submit</code>时加上<code>—verbose</code>来打印参数的来源或者在Spark Web UI上查看使用的设置。</p>
<ul>
<li>conf目录下（低）<ul>
<li><code>spark-default.xml</code>：这两个文档中对参数有详细说明。<a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark配置文档</a>，<a href="http://spark.apache.org/docs/latest/running-on-yarn.htm" target="_blank" rel="noopener">Yarn配置文档</a>，<a href="http://spark.apache.org/docs/latest/monitoring.html#spark-configuration-options" target="_blank" rel="noopener">Moniror配置文档</a></li>
<li><code>spark-env.sh</code>： <a href="http://spark.apache.org/docs/latest/configuration.html#environment-variables" target="_blank" rel="noopener">Spark配置文档环境变量一节</a>，<a href="http://spark.apache.org/docs/latest/monitoring.html#environment-variables" target="_blank" rel="noopener">Monirot的配置环境变量一节</a>。配置<code>HADOOP_CONF_DIR</code>，<code>HIVE_CONF_DIR</code>这些常用的变量。</li>
<li><code>log4j.properties</code>： 控制日志输出的等级。在conf下有模板。参考 <a href="http://logging.apache.org/log4j/" target="_blank" rel="noopener">log4j</a> 的官方文档。</li>
</ul>
</li>
<li><code>spark-submit</code>的参数（中）<ul>
<li>命令行选项opition：如<code>--master</code>、<code>--name</code>等，基本都可以在<code>—conf</code>中找到对应的配置。<strong>在上面的文档的表格<code>Meaning</code>中可以找到对应的opinion</strong>。</li>
<li><code>--conf&lt;key&gt;=&lt;value&gt;</code>方式：有很多内置的key，如<code>spark.app.name</code>、<code>spark.master</code>。<strong>与<code>spark-default.xml</code>中的配置的key一致。</strong></li>
</ul>
</li>
<li><code>SparkConf</code> 代码设定(高)：<code>val conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;CountingSheep&quot;) val sc = new SparkContext(conf)</code><ul>
<li>​</li>
</ul>
</li>
</ul>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode client \  # 或者cluster</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  --queue spark-default # Yarn模式下 指定队列</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt;</span><br><span class="line">  --jars PATH/utils.jar # 制定了第三方的jar包</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \ # 指定jar包中运行的Main Class</span><br><span class="line">  /path/to/examples.jar \ #&lt;application-jar&gt;</span><br><span class="line">  1000 #[application-arguments]</span><br></pre></td></tr></table></figure>
<ul>
<li>spark-submit 运行Application的基本要素<ul>
<li><code>--master yarn</code> 指定启动模式，如<ul>
<li><code>local</code>（单核）、<code>local[2]</code>、<code>local[*]</code>(依据cpu设定)：注意使用单核local模式时比较特殊，一下逻辑可以正常运行，不代表在多核或者其他模式下可以运行，因此此模式仅供测试。</li>
<li><code>yarn</code>：最<strong>常用</strong>的模式，在yarn上运行代码，配置hadoop文件的位置在<code>HADOOP_CONF_DIR</code>或者<code>YARN_CONF_DIR</code>中。改变量在<code>conf/spark-env.sh</code>中配置</li>
<li>其他地址:<code>spark://spark_alone_mode_addr</code>,<code>mesos://mesos_mode_addr</code></li>
</ul>
</li>
<li><code>--deploy-mode client</code>：运行模式。<strong>使用spark-submit提交job默认是<code>client</code></strong>。对于一般短期例行应用，使用client模式配合调度系统（如Azkaban）的retry机制即可。备选<code>cluster</code>模式可用。</li>
<li><code>--calss com.package.xxxClass</code> ：指定运行的main class </li>
<li><code>&lt;application-jar&gt;</code> &amp;&amp; <code>[application-arguments]</code>：注意package时不要提交spark、hadoop自身，较少包体积。参数可以通过main的args读取。当然，也可以使用<code>—conf</code>来传参.</li>
</ul>
</li>
<li>其他选项<ul>
<li>指定与提交依赖（常用）：经常有写第三方或者工具jar要调用，可以用这个方式。应当注意的是<strong>必须确保executors和driver必须能访问到该jar文件</strong>（如共享或者copy）<ul>
<li><code>—-jars path/xxx.jar,path/xxx2.jar</code>： 多个jar则使用<code>,</code>分割。path支持<ul>
<li><code>local</code>：本地文件目录，注意，必须手动（或者由调度系统）<strong>copy相同的jar到所有机器的相同目录下面</strong>。当然也可以是NFS这些共享文件系统的目录。</li>
<li><code>file://</code>：driver的绝对路径，从Driver内置的HTTP Server复制过去</li>
<li><code>hdfs://</code>：从hdfs文件系统访问</li>
<li><code>http://</code> <code>https://</code> <code>ftp://</code>：其他可以访问的url</li>
</ul>
</li>
<li>通过maven 依赖方式:<ul>
<li><code>—packages</code></li>
<li><code>—repositories</code></li>
</ul>
</li>
<li>其他注意点：jar的复制可能会占用不少空间，yarn模式会自动清理这些jar。其他模式请手动处理。</li>
</ul>
</li>
<li><strong>指定queue</strong>（Yarn方式特有）： <code>--queue spark-default</code></li>
<li>spark-submit的其他option参数设定<ul>
<li><code>--num-executors 20</code></li>
<li><code>--executor-memory 20G</code></li>
<li><code>--driver-memory 3G</code></li>
<li><code>--executor-cores 3</code></li>
<li>..参考文档中其他的配置。</li>
</ul>
</li>
<li><code>--conf &lt;key&gt;=&lt;value&gt;</code> 。<ul>
<li>添加系统配置。</li>
<li>可以通过添加多个 <code>--conf</code>设置多个配置</li>
<li>自定义参数<ul>
<li><code>--conf spark.test.arg.mypath=hdfs://xxx</code> 自定义参数。</li>
<li>获取<code>String value = System.getProperty(&quot;spark.test.arg.mypath&quot;);</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>jvm控制变量<ul>
<li><code>-conf &quot;spark.driver.extraJavaOptions =-XX:+PrintGCDetails&quot;</code>:给Driver的JVM设置参数，注意不能设置heap内存这些，要通过spark.driver.memory来设置。<strong>Client模式下不适用，因为Driver的JVM此时已经启动了，使用—driver-java-options设置</strong>。</li>
<li><code>-conf &quot;spark.executor.extraJavaOptions =-Dlog4j.configuration=file_path&quot;</code>：给Executor的JVM设置参数</li>
<li><code>--driver-java-options &quot;-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8&quot;</code>：给Client模式的Driver设置虚拟机参数，常常用在设置spark-shell的字符编码.</li>
</ul>
</li>
<li>spark-shell：本质上还是spark-submit，配合了一个scala的shell。也可以传递依赖的jar包<ul>
<li>命令行字符问题处理<code>--driver-java-options &quot;-Dfile.encoding=UTF-8 -Dsun.jnu.encoding=UTF-8&quot;</code></li>
<li>jar包上传</li>
</ul>
</li>
</ul>
<h3 id="配置的介绍"><a href="#配置的介绍" class="headerlink" title="配置的介绍"></a>配置的介绍</h3><ul>
<li>应用配置：应用模式，名称，内存</li>
<li>运行环境：java环境，库</li>
<li>shuffle相关</li>
<li>sparkUI的设置</li>
<li>序列化和压缩</li>
<li>内存管理</li>
<li>执行器Executor行为</li>
<li>网络</li>
<li>调度schedule，资源分配</li>
<li>安全，加解密</li>
<li>SparkStreaming</li>
</ul>
<h2 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h2><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD（Resilient Distributed Dataset）是<strong>不可变</strong>的分布式<strong>对象集合</strong>。是Spark最重要的概念，是对分布式数据的一个抽象表示。重点理解：</p>
<ul>
<li>不可变：对RDD的每一次操作都会返回新的RDD，不会修改原有的集合。</li>
<li>对象集合：RDD中包含了<strong>同一种类型</strong>的对象，如String的RDD既RDD中的所有元素都是String。</li>
<li>惰性求值：只有执行Action时，才会真正运行一系列操作（包括读入数据）。</li>
<li>RDD包含5个重要属性：分区表，一个用于计算每个split的函数（具体逻辑？），依赖的其他RDD的list，key-value RDD需要一个Partitioner（可选），计算每个split时优先使用的location（数据本地化，可选），<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">参考</a>。</li>
</ul>
<h4 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h4><ul>
<li>直接从本地文件，HDFS上的文件上创建 <code>val textFile =sc.textFile(&quot;README.md&quot;)</code>。具体API和支持的文件类型参考IO一节。</li>
<li>从内存集合数据创建 <code>val distData = sc.parallelize(Array(1, 2, 3, 4, 5))</code>。一般用于测试，例如：使用File打开一个文件读入到内存，然后转换成RDD。</li>
</ul>
<h4 id="RDD的分类（重点）"><a href="#RDD的分类（重点）" class="headerlink" title="RDD的分类（重点）"></a><u>RDD的分类（重点）</u></h4><ul>
<li>普通RDD：既包含了任意某种类型对象的RDD。如RDD[String]，具有统一的Transform和Action操作。</li>
<li>特殊类型的RDD：RDD的元素是特定的类型的RDD，除了通用方法外还有其他方法。<ul>
<li><a href="http://www.scala-lang.org/api/2.11.7/index.html#scala.Tuple2" target="_blank" rel="noopener">Tuple2</a>对象：即Key-Value对象的RDD，通过Scala隐式转换使它具有很多特有的方法。如ReduceByKey。<ul>
<li>如果key实现了Ordered，那么还有sortByKey操作</li>
</ul>
</li>
<li>Double数值对象：RDD[Double]，数值类型在RDD中表示为Double。具有一下统计相关的特有方法，如计算均值。</li>
</ul>
</li>
</ul>
<h4 id="核心API"><a href="#核心API" class="headerlink" title="核心API"></a>核心API</h4><p>参考官方<a href="http://spark.apache.org/docs/latest/api/scala/index.html#package" target="_blank" rel="noopener">API手册</a></p>
<blockquote>
<p>技巧：由于<strong>Scala的隐式转换</strong>特性，查找API文档时需要注意，对于RDD的一下操作，除了<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">RDD</a>中通用操作，还有一下其他的类需要查看<strong>XXRDDFunctions</strong>相关的类，如<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.DoubleRDDFunctions" target="_blank" rel="noopener">DoubleRDDFunctions</a>、<a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html" target="_blank" rel="noopener">PairRDDFunctions</a>。</p>
</blockquote>
<ul>
<li>通用 RDD<ul>
<li>Transform<ul>
<li>针对每个元素的操作：<ul>
<li>map：不改变元素的个数</li>
<li>filter：元素的个数小于等于old</li>
<li>flatMap：元素的个数大于等于old</li>
</ul>
</li>
<li>伪集合操作：<ul>
<li>distinct</li>
<li>union：并集，结果包含重复数据</li>
<li>intersection：交集，结果自动distinct了。需要shuffle</li>
<li>subtract：差集，需要shuffle</li>
<li>certesian：笛卡儿积，即所有（a,b）对。</li>
</ul>
</li>
<li>其他<ul>
<li>sample：生成采样的RDD（还是分布式的，区分takeSample）</li>
</ul>
</li>
</ul>
</li>
<li>Action<ul>
<li>数据整合<ul>
<li>reduce：例如<code>rdd.reduce((x,y)=&gt;x+y)</code></li>
<li>fold：与reduce类似，但是提供初始值，例如<code>rdd.fold(0)((x,y)=&gt;x+y)</code></li>
<li>aggregate：与reduce类型，但是<strong>可以返回不同的数据类型</strong>。注意参数除了初始值外，需要提供两个函数分别用于集合内部value合并&amp;&amp;集合之间合并。demo参考《Spark快速大数据分析》的p35</li>
</ul>
</li>
<li>count：计算RDD中元素的个数</li>
<li>countByValue：对<strong>元素内容统计计数</strong></li>
</ul>
</li>
<li>foreach：对每个函数调用某个方法。注意与map不同，<strong>它没有副作用（返回RDD）</strong>。典型的例子是用在把每个元素调用接口发送到其他地方。</li>
<li>collect/take(n)/top/takeSample/takeOrdered:返回驱动器的一系列操作，一般用于调试目的。</li>
</ul>
</li>
</ul>
<ul>
<li>Key-Value RDD <ul>
<li>pairRDD的生成<ul>
<li>某些文件读取的就是pairRDD</li>
<li>通过map处理后生成</li>
</ul>
</li>
<li>Transform<ul>
<li>单个pairRDD的方法<ul>
<li>reduceByKey：与reduce区分，这里是Transform操作，生成了一个RDD，key是以前的key，value是这个key下reduce计算的结果。</li>
<li><strong>combineByKey</strong>：reduceByKey的加强版。可以返回与输入不同类型的值。参数需要提供3个函数，1.为某个key的提供初始值的函数，2.该key集合内部value合并 3.该key集合之间合并。demo参考《Spark快速大数据分析》的p47</li>
<li>groupByKey：按key集合。key是以前的key，value是一个list，包含了这个key下的所有的value。</li>
<li><strong>mapValues</strong>：保留Key，值修改Values，注意，这个方法很重要，它保留了原来RDD的分区信息。</li>
<li>flatMapValues</li>
<li><strong>mapPartitions</strong>：一个分区调用一次该函数，参数是一个迭代器。</li>
<li>keys/values：返回所有key/value</li>
</ul>
</li>
<li>用于两个pairRDD的方法<ul>
<li>join</li>
<li>rightOuterJoin/leftOuterJoin：右连接、左连接</li>
<li><strong>subtractByKey</strong>：删除调key相同的元素</li>
<li>cogroup：按照相同的的key分组。注意结果中key是原来的key，<strong>value是一个元组</strong>，类似于先对两个RDD使用group，然后join。</li>
</ul>
</li>
<li>其他<ul>
<li>sortByKey，对于key是Ordered的，可以对key排序。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Double RDD<ul>
<li>stats()：返回StatsCounter对象，包含了一系列统计值，如min max sum mean等</li>
<li>mean，sum</li>
<li>variance</li>
</ul>
</li>
</ul>
<h4 id="核心概念（重点）"><a href="#核心概念（重点）" class="headerlink" title="核心概念（重点）"></a>核心概念（重点）</h4><ul>
<li>partition分区的<strong>方式（只有Key-Value RDD可以设置Partitioner）</strong>与<strong>数目</strong><ul>
<li>默认的读入的数据的分区数目<ul>
<li>内存数据：与cpu核心数目相同。</li>
<li>文件数据：与block块数相同</li>
</ul>
</li>
<li>默认读入数据的分区方式partitioner（Key-Value RDD）：None</li>
<li>获取分区数目：<code>rdd.partitions.size</code></li>
<li>设置分区数目：<ul>
<li><code>rdd.repartition(100)</code></li>
<li><code>rdd.coalesce(100)</code>：<strong>没有shuffle操作</strong>，适用于将多分区变少的情况。如果shuffle = true，效果同上。</li>
</ul>
</li>
<li>获取分区方式partitioner（Key-Value RDD）：<code>rdd.partitioner</code></li>
<li>设置分区类型partitioner（Key-Value RDD）<ul>
<li>方法一：<code>rdd.partitionBy(new spark.HashPartitioner(2))</code>会返回混洗后的RDD</li>
<li>方法二：一些操作具有可选的Partitioner作为参数，如<code>join()</code>、<code>groupByKey()</code>，操作返回的RDD具有次Partitioner</li>
</ul>
</li>
<li>设置新的分区方式Partitioner(rdd.partitionBy)，包含了shuffle的步骤，如果想保留效果<strong>需要Cache这个RDD，否则没有意义。</strong></li>
<li>自定义分区方式：继承<code>Partitioner</code>类，实现对应方法</li>
<li>各种操作对分区的影响<ul>
<li>添加新的分区类型：<ul>
<li>哈希分区：<ul>
<li>reduceByKey、CombineByKey、groupByKey</li>
<li>二元操作：join、leftOuterJoin、rightOuterJoin、、cogroup(groupWith)</li>
</ul>
</li>
<li>范围分区：sortByKey</li>
<li>添加指定类型：partitionBy</li>
</ul>
</li>
<li><strong>破坏分区类型</strong>：map</li>
<li>保留父RDD的分区方式：filter、mapValues、flatMapValues</li>
<li>对于二元操作：<ul>
<li><strong>默认结果是Hash分区，分区数与操作的并行度相同。</strong></li>
<li>如果一个父RDD设置过Partitioner，那么结果会采用它的分区方式</li>
<li>如果两个RDD都设置了Partitioner，那么结果使用第一个的分区方式。</li>
</ul>
</li>
</ul>
</li>
<li>分区的应用<ul>
<li>增加速度—大的数据集需要多次与不同的小数据集join的情况，<strong>对大数据集设置Partitioner，再Join，使得shuffle中只传递小的数据集。</strong></li>
<li>使用mapValues代替map，防止丢失Partitioner信息</li>
</ul>
</li>
</ul>
</li>
<li>Cache问题<ul>
<li>默认情况下，Spark不会在内存中保留数据，第二次使用RDD时（如一个新的Aciton操作涉及到了以前的RDD），会重新计算这个RDD。对于计算RDD很费资源的情况，这是十分低效。</li>
<li>使用cache()或者persist(MEMORY_ONLY)缓存到内存：默认情况下，RDD对象会使<strong>用非序列化的格式直接缓存对象</strong>。优点是下次使用时很快。缺点是比较浪费空间。<strong>如果内存无法容纳，则会部分partition会不被缓存</strong>。</li>
<li>persist的参数可选的有以序列化的方式缓存到内存/只缓存到disk/内存和disk混合使用，<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">参考这里</a>：对于十分耗时的RDD的运算，建议内存disk混合缓存，确保所有内存均被缓存。</li>
<li>有些情况下可能缓存失效（如机器问题），可以缓存多份。在存储级别后面加上’_2’</li>
<li>unpersist可以取消缓存。不调用这个方法默认会以LRU的方式处理。</li>
</ul>
</li>
</ul>
<h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4><ol>
<li>回调函数引用问题：RDD的操作函数中有大量的传入回调函数的情况（如map），由于回调函数会发送到Executor中去运行，所以需要注意回调函数的引用外层对象<code>this</code>的问题，会出现<code>NotSerializableException</code>的错误。参考<a href="http://spark.apache.org/docs/latest/programming-guide.html#passing-functions-to-spark" target="_blank" rel="noopener">文档</a>，总结一下，推荐使用下面两种形式：<ul>
<li>使用匿名回调函数作为参数</li>
<li>使用object（全局对象）中的函数作为参数</li>
<li>注意：<strong>函数中如果引用类的成员变量，请先copy到层本地变量</strong>，如：<code>val field_ = this.field</code></li>
</ul>
</li>
<li>map与mapValues对分区的影响</li>
<li><strong>使用rdd.coalesce的一个坑</strong>：如果我们的代码类似rdd.map(xxx)….coalesce(1).write…这样的操作吧结果输出到一个文件中，活导致先合并在执行map操作的问题。这是由于spark的<strong>stage合并</strong>机制导致的，<a href="https://qnalist.com/questions/4981936/partitions-coalesce-and-parallelism" target="_blank" rel="noopener">参考文章</a>.</li>
<li>如同类型的RDD之间的隐式转化：在spark1.3.0之后版本，无需使用<code>import org.apache.spark.SparkContext._</code>，RDD会自动转换转换对应XXRDDFunctions。</li>
</ol>
<h3 id="I-O方式"><a href="#I-O方式" class="headerlink" title="I/O方式"></a>I/O方式</h3><p>讨论一下Spark读入文件的方式。一般我们需要的处理的数据都是位于某个文件系统上的某个（些）文件。Spark可以访问下面这些数据。</p>
<ul>
<li>文件格式<ul>
<li>文本文件<ul>
<li>read:<code>sc.textFile(&quot;path&quot;)</code></li>
<li>write:<code>sc.saveAsTextFile(&quot;path&quot;)</code></li>
</ul>
</li>
<li>json/逗号制表符分割的文本(半结构化)等特殊的文本格式<ul>
<li>read：textFile读入数据，然后使用json/csv库在mapPartition中解析出对象</li>
<li>write：类似上面，用json/csv库解析成字符串，然后saveAsTextFile。</li>
</ul>
</li>
<li><strong>SequenceFile结构化Key-Value文件</strong>：要求key、value都是Writable对象<ul>
<li>read：<code>sc.sequenceFile[Key,Value](&quot;path&quot;,minPartitions)</code>。获得key-value RDD（这里自动把Writable对象转换成Scala对象）。等价于<code>sc.sequenceFile(&quot;path&quot;,classOf[Text],classOf[IntWritable]).map{case(x,y)=&gt;(x.toString,y.get())}</code></li>
<li>write：<code>data.saveAsSequenceFile(&quot;path&quot;)</code>。注意data是个key-value的RDD</li>
</ul>
</li>
<li>对象文件：不一定要是key-value。可以是简单的值RDD。<strong>一般用于job之间的通讯？</strong><ul>
<li>read：sc.objectFile()</li>
<li>write：sc.saveAsObjectFile</li>
</ul>
</li>
</ul>
</li>
<li>文件系统<ul>
<li>本地文件系统、nfs：<code>file:///home/name/xxx.txt</code></li>
<li>HDFS：<code>hdfs://master:port/path</code></li>
<li>Amazon S3：<code>s3n://bucket/….</code></li>
</ul>
</li>
<li>非文件系统存储系统/数据库<ul>
<li>JDBC数据库 </li>
<li>HBase：<ul>
<li>使用<code>hadoopRDD</code>/<code>saveAsHadoopDataSet</code> (<code>newAPIHadoopRDD</code>/<code>saveAsnewAPIHadoopDataSet</code>)来读取，与<strong>hadoopFile区别是没有path这个参数</strong></li>
</ul>
</li>
<li>MongoDB\Elasticsearch</li>
</ul>
</li>
<li>SparkSQL读取结构化数据源（带有Schema，或者可以推断出来）：参考下一节</li>
<li>支持使用Hadoop <code>InputFormat</code>和<code>OutputFormat</code>接口访问的所有数据（如HDFS，S3，HBase）。这种方式比较原始，不推荐直接使用，推荐用更高层的API。<ul>
<li>注意点：<ul>
<li>hadoop的<code>Inputformat</code>必须是键值对（key-value）的形式，<strong>如果没有key，则使用假键null</strong>。</li>
<li>有新旧两套API<ul>
<li>旧的<code>sc.hadoopFile[Text,Text,xxInputFormat](inputfile)</code></li>
<li>新的<code>sc.newAPIHadoopFile(intputfile,classOf[xxInputFormat],classOf[Text],classOf[Text],conf)</code></li>
</ul>
</li>
</ul>
</li>
<li>read：<code>sc.newAPIHadoopFile</code>、<code>sc.newAPIHadoopFile</code></li>
<li>write：<code>data.saveAsHadoopFile()</code>、<code>data.saveAsNewAPIHadoopFile()</code></li>
<li>应用，<ul>
<li>读取pb数据 — 《Spark快速大数据分析》p75</li>
<li>文件压缩 — 《Spark快速大数据分析》p74 p77常见的文件压缩格式（注意<strong>是否可分割</strong>）</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>一个Q&amp;A：<strong>哪些接口是基于原始接口（hadoopfile）构造出来的高层接口?</strong></p>
</blockquote>
<h3 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h3><h4 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h4><ul>
<li>理解：由于Spark的运行原理，变量只具有局部性。累加器提供了一个全局变量的功能。</li>
<li>使用<ul>
<li><code>val acc=SparkContext.accumulator(0)</code>初始化，参数的类型自动推断累加器的类型，这里是<code>Int</code></li>
<li>设置 <code>acc+=1</code></li>
<li>在driver中读取：<code>acc.value</code></li>
</ul>
</li>
<li>自带的累加器：<code>Int</code>,<code>Double</code>,<code>Long</code>,<code>Float</code></li>
<li>自定义累加器：扩展AccumulatorParam</li>
<li>应用场景<ul>
<li>统计读入数据的错误行数。</li>
</ul>
</li>
<li>注意点<ul>
<li><strong>如果有节点失败，在Transfer操作里面的累加器可能重复增加</strong>，但是实际应用中依据业务需求这个问题不严重。</li>
<li>cache也会对Transfer操作的累加的结果有影响</li>
<li><strong>可靠的作法是在Action操作中使用累加器。如foreach。</strong></li>
<li>​</li>
</ul>
</li>
</ul>
<h4 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h4><ul>
<li>理解：由于Spark会分发任务执行，如果我们引用某个变量（只读），那么<strong>每次执行操作时都会下发这个变量的值</strong>。某些变量很大的情况下（如查询表），重复分发浪费带宽，可以用广播变量。优点是它之后分发一次，以后Executor节点可以直接只用。</li>
<li>使用：<ul>
<li><code>val bd=SparkContext.broadcast(variable)</code></li>
<li>使用时，<code>bd.value</code></li>
</ul>
</li>
<li>应用场景<ul>
<li>查找表</li>
<li>机器学习总大的特征向量</li>
</ul>
</li>
<li>注意<ul>
<li><strong>一定要是只读常量</strong>，对它的修改不会重新分发。</li>
<li>一般几百KB到几MB的变量使用广播，不要滥用。</li>
<li>如果广播的变量比较大，<strong>对性能有一些影响，体现在变量序列化的时间上</strong>，推荐使用Kryo方式序列号。</li>
</ul>
</li>
</ul>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><h3 id="理解RDD-DataFrame-DataSet几种API的本质区别"><a href="#理解RDD-DataFrame-DataSet几种API的本质区别" class="headerlink" title="理解RDD DataFrame DataSet几种API的本质区别"></a>理解RDD DataFrame DataSet几种API的本质区别</h3><ul>
<li>RDD—需要序列化（Java/ Kryo序列化），因此，即使只查询1列数据也要序列化整个对象（代表一行所有列）。而且所有数据都要存放在堆内存中，因此性能差。而且序列化的obj占用对象很大，每个对象都要存储数据+结构。（对比schema的方式）。优点是代码好写，支持编译时的检查。</li>
<li>DataFrame—存储使用schema+数据的方式。数据还可以列式存储增强性能。而且可以在javaheap外操作。（与java对象无关）。缺点也很明显，代码不好写，只能运行时检测（和sql类似）</li>
<li>DataSet，两者优点结合，但是需要一个Encoder。注意：虽然Encoder的作用也是把对象转换成bytes，但是<strong>与Java/ Kryo序列化不同，和Encoder的优势是：</strong>可以不反序列化成Object的情况下进行一些常规操作，如排序，过滤，hash。</li>
<li>推荐文章<ul>
<li><a href="http://www.jianshu.com/p/c0181667daa0" target="_blank" rel="noopener">RDD、DataFrame和DataSet的区别</a>：Dataset内部一些优化</li>
<li>官网介绍：<a href="https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html" target="_blank" rel="noopener">Introducing Apache Spark Datasets</a></li>
</ul>
</li>
</ul>
<h3 id="核心API-1"><a href="#核心API-1" class="headerlink" title="核心API"></a>核心API</h3><ul>
<li><p>入口</p>
<ul>
<li>Spark1.6：sqlContext/HiveContext(带有Hive功能)</li>
<li>Spark2.0：sparkSession</li>
</ul>
</li>
<li><p>创建DF/DataSet</p>
<ul>
<li><p>从RDD创建</p>
<ul>
<li><p><strong>case class/Product类方式，隐式转换</strong>。注意<strong>必须使用</strong><code>import spark.implicits._</code> 再调用<code>rdd.toDF()</code>。需要注意的是，一般类型，或者<strong>嵌套<code>Array</code>/<code>Seq</code>都是可以推断出来</strong>的。</p>
<blockquote>
<p>Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,you can use custom classes that implement the Product interface</p>
</blockquote>
</li>
<li><p>显式指定StructType的方式。参考API文档，比较复杂的情况下使用。RDD—&gt;Row的RDD—&gt;<code>spark.createDataFrame(rowRDD, schema)</code></p>
</li>
</ul>
</li>
<li><p>IO方式：读取的是DataFrame（即DataSet[Row]）参见下一节</p>
</li>
<li><p>内存数据如Seq：<code>Seq(Person(&quot;Andy&quot;, 32)).toDS()</code></p>
</li>
<li><p>DataFrame—&gt;DataSet</p>
<ul>
<li>Map方式：把<strong>Row对象</strong>转换成对应的对象</li>
<li>IO方式读取数据源时+as：<code>val peopleDS = spark.read.json(path).as[Person]</code></li>
</ul>
</li>
<li><p>自定义Encoder：<code>implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]</code></p>
</li>
</ul>
</li>
<li><p>RDD类似API</p>
<ul>
<li>map</li>
<li>filter</li>
<li>join</li>
</ul>
</li>
<li><p>表相关的api</p>
<ul>
<li>select<ul>
<li>使用sql语句：<code>sqlContext.sql(&quot;select count(name) as name_cnt from one_table&quot;)</code></li>
<li>使用select函数：<code>df.select(&quot;count(name) as name_cnt&quot;)</code></li>
<li>使用批量的select执行函数·:<code>val rrs = df.selectExpr(select_expr_list: _*)</code>。其中select_expr_list代表多个语句的list。如<code>val list=List(&quot;count(name) as name_cnt&quot;,&quot;xxudf(col_name) as xxflag&quot;)</code></li>
</ul>
</li>
<li>drop</li>
<li>groupBy&amp;&amp; agg聚合函数</li>
<li>withColumn<ul>
<li>添加常量列</li>
</ul>
</li>
</ul>
</li>
<li><p>sql查询API</p>
<ul>
<li>新建表：<code>df.createOrReplaceTempView(&quot;table_name&quot;)</code>或者就API<code>df.registerTempTable(&quot;table_name&quot;)</code></li>
<li>执行查询 <code>sqlContext.sql(&quot;sql expression&quot;)</code></li>
</ul>
</li>
<li><p>UDF：用户自定义函数，他的param是列Column，return是新Colomn的内容。写复杂逻辑代码用的很多！</p>
<ul>
<li>定义&amp;&amp;使用UDF的方式一：<ul>
<li><code>val my_udf=udf[String,String]( x =&gt; x+&quot;end&quot; )</code>:使用udf函数定义（该函数在function中）</li>
<li><code>df.withColume(my_udf(col(&quot;col_1&quot;)))</code></li>
</ul>
</li>
<li>定义&amp;&amp;使用UDF的方式二：<ul>
<li><code>spark.udf.register(&quot;my_udf&quot;,x=&gt;x+&quot;end&quot;)</code></li>
<li>spark.sql(“select my_udf(‘col_1’) from table “)在sql中使用</li>
</ul>
</li>
</ul>
</li>
<li><p><code>org.apache.spark.sql.functions</code>的built-in函数：一些帮助方法，主要与生成新的Col有关，<strong>这些函数大大方便了开发</strong>。</p>
<ul>
<li>功能<ul>
<li>构建Column<ul>
<li><code>col(&#39;name&#39;)</code>：使用列名没返回Column对象</li>
<li><code>lit(num/String)</code>:生成常量列</li>
</ul>
</li>
<li>构建UDF：<code>udf(f: FunctionN[...])</code></li>
<li>agg函数<ul>
<li>count/distinctCount</li>
<li>min/max</li>
<li>sum/mean</li>
<li>first/last</li>
</ul>
</li>
<li>normal 函数（理解成内置的UDF）<ul>
<li>String相关：split(col)/upper(col)/lower(col)</li>
<li>数值相关：sin(col)/cos(col)/sqrt(col)/rand()等等</li>
<li>表达式解析：expr(sql)。依据sql语句执行生成新的col <code>df.filter(expr(&quot;token = &#39;hello&#39;&quot;))</code></li>
</ul>
</li>
<li>排序相关，与df.sort函数配合使用：<ul>
<li>asc(col)</li>
<li>desc(col)</li>
</ul>
</li>
<li>window函数:????<ul>
<li>lag</li>
<li>lead</li>
<li>rank</li>
</ul>
</li>
</ul>
</li>
<li>使用<ul>
<li>使用<code>import org.apache.spark.sql.functions._</code>导入</li>
</ul>
</li>
<li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-functions.html" target="_blank" rel="noopener">参考文档</a>，<a href="https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/sql/functions.html" target="_blank" rel="noopener">API文档</a></li>
</ul>
</li>
<li><p>DF的Map操作问题！！！</p>
<ul>
<li><p>旧版本DataFrame时，map后又变成了RDD</p>
</li>
<li><p>新版由于DataFrame本质就是DataSet，map操作就是DataSet的map。它的返回也是一个DataSet，因此要求map的函数返回值必须是可以序列化的！！或者自定义encoder</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="type">Encoders</span>.kryo[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>]]</span><br><span class="line">teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>)))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>RDD to DataSet/DF ：</p>
<ul>
<li>使用Product，<code>import spark.implicits._</code></li>
</ul>
</li>
</ul>
<h3 id="I-O方式-1"><a href="#I-O方式-1" class="headerlink" title="I/O方式"></a>I/O方式</h3><ul>
<li>最大不同，以Spark SQL的方式读取数据<strong>不是全表扫描，而是按需读取需要字段</strong>。（与SparkContext.hadoopFile不同）</li>
<li>文件格式：必须是<strong>结构化</strong>的数据源，<ul>
<li>parquet（有schema），一直流行的<strong>嵌套</strong>式<strong>列式</strong>存储格式</li>
<li>hive表，</li>
<li>json（可推断），</li>
<li>jdbc（关系型有表结构）</li>
</ul>
</li>
<li>API<ul>
<li>读<ul>
<li><code>spark.read.format(&quot;json&quot;).load(path)</code>  其他格式：”parquet”</li>
<li><code>spark.read.json(path)</code>或者<code>spark.read.parquet(path)</code></li>
</ul>
</li>
<li>写<ul>
<li><code>df.write.format(&quot;json&quot;).save(path)</code> 其他格式：”parquet”</li>
<li><code>df.write.json(path)</code>或者<code>df.write.parquet(path)</code></li>
</ul>
</li>
<li>Hive<ul>
<li>配置通过hive-site.xml，并构造HiveContext，可以直接<code>hiveContext.sql(&quot;sql&quot;)</code></li>
<li>读写通过sql语句实现</li>
</ul>
</li>
<li>旧版本（1.4以前）<ul>
<li><code>sqlContext.parquetFile</code>, <code>sqlContext.jsonFile</code></li>
</ul>
</li>
</ul>
</li>
<li>性能<ul>
<li>内存式的列式存储</li>
<li>谓词下堆：不需要读取整个数据集，根据值的特点选择性读取（如数值范围）</li>
</ul>
</li>
<li>注意点：<ul>
<li>json读取方式的性能问题，由于json的格式推断，导致<strong>读取大量数据非常非常慢</strong>！因为要全局推断它的schema。</li>
</ul>
</li>
</ul>
<h3 id="常见需求最佳实践"><a href="#常见需求最佳实践" class="headerlink" title="常见需求最佳实践"></a>常见需求最佳实践</h3><ol>
<li><p>活用UDF：对于一些自定义要求很高的需求，我们最常见的思路是：使用UDF解决。通常配合下面的函数</p>
<ul>
<li><strong>withColumn方法</strong>：新增一列，可以使用udf对多个列进行计算，然后生成新列。<code>df.withColumn(&quot;new_col&quot;,my_udf(col(&quot;col1&quot;),col(col2)))</code></li>
<li>select方法：提取某些列，然后使用udf计算，<strong>不保留原来的表</strong>。 <code>df.select(my_udf(col(&quot;col1&quot;),col(&quot;col2&quot;)) as &quot;my_name&quot;, col(&quot;other_col&quot;))</code></li>
<li><strong>filter方法</strong>：比较复杂的过滤。<code>df.filter(my_udf(col(&quot;col1&quot;),col(&quot;col2&quot;)))</code>。这里的my_udf返回True/False</li>
<li>注意：<strong>groupBy之后暂时还不能用UDF，应当使用UDAF。</strong></li>
</ul>
</li>
<li><p>单表group操作：应当注意的是<strong>表的GroupBy操作，返回的是GroupedData对象，不是DataFrame</strong>。后面必须接上聚合操作才能返回聚合后的DataFrame：</p>
<ul>
<li>常见操作，只计算一个值。df.groupBy(“col1”,”col2”).count/max/min/sum/mean()</li>
<li>复杂些的，计算多个值：df.groupBy(“col1”,”col2”).agg()<ul>
<li><code>df.groupBy(&quot;col1&quot;,&quot;col2&quot;).agg(max(&quot;age&quot;),sum(&quot;expense&quot;))</code></li>
<li><code>df.groupBy(&quot;col1&quot;,&quot;col2&quot;).agg(Map(&quot;age&quot;-&gt; &quot;max&quot;, &quot;expense&quot; -&gt;&quot;sum&quot;))</code></li>
</ul>
</li>
<li>自定义agg操作：UDAF，新功能。</li>
</ul>
</li>
<li><p>多表join（filter in another table）：<code>df1.join(df2,condition_express,&quot;type_string&quot;)</code></p>
<ul>
<li>join的三种方式。参考<a href="http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/" target="_blank" rel="noopener">Beyond traditional join with Apache Spark</a><ul>
<li>默认只包含公共的key:inner</li>
<li>left_outer/right_outer</li>
<li>fullouter</li>
</ul>
</li>
<li>join之后的列名字<ul>
<li>先重命名withColumnRenamed，再join</li>
<li>直接显示指定df1(“col”)==df2(“col”)</li>
</ul>
</li>
<li>空值问题==&gt;DataFrame的na方法</li>
<li>多列相等的join</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 显示指定col	 </span></span><br><span class="line"><span class="keyword">val</span> res = df1.join(df2,df1(<span class="string">"uid"</span>) === df2</span><br><span class="line">      (<span class="string">"uid"</span>) &amp;&amp; df1(<span class="string">"time"</span>) === df2</span><br><span class="line">      (<span class="string">"time"</span>),<span class="string">"left_outer"</span>).</span><br><span class="line">      na.fill(<span class="type">Map</span>(</span><br><span class="line">      <span class="string">"click_cnt"</span> -&gt;<span class="number">0</span></span><br><span class="line">    )).drop(df2(<span class="string">"uid"</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>寻找不在另外一个表中的行(filter not in another table)</p>
<ul>
<li><code>df1.join( df2.select($&quot;id&quot;.alias(&quot;id_&quot;)), $&quot;id&quot; === $&quot;id_&quot;, &quot;leftouter&quot;) .where($&quot;id_&quot;.isNull).drop(&quot;id_&quot;)</code></li>
<li>如果另外一个集合比较小，可以考虑用filter配合UDF过滤set(可能还需要broadcast)</li>
</ul>
</li>
<li><p>空值处理：DataFrame的na方法</p>
<ul>
<li>fill</li>
<li>drop</li>
<li>replace</li>
</ul>
</li>
<li><p>使用lit方法生成常量列，方便统计一些值。</p>
</li>
<li><p>同时Group多组col，并统计行数（count操作）</p>
<ul>
<li>问题：使用groupby方法只能对一组col进行group操作，如’name’&amp;&amp;’age’，如果要同时对’name’&amp;&amp;’sex’进行group，这需要再对df进行一次group。如何对df进行一次操作就可以完成多组的group并输出结果呢？</li>
</ul>
</li>
</ol>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>简单介绍一下，以后详细介绍</p>
<ul>
<li>mini-batch的模式</li>
<li>适合1s以及以上的实时性</li>
<li>api与批量具有相似性</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>调用其他语言程序—pipe 《Spark快速大数据分析》p97</li>
<li>安全问题</li>
</ul>
<h2 id="性能优化的基本技巧"><a href="#性能优化的基本技巧" class="headerlink" title="性能优化的基本技巧"></a>性能优化的基本技巧</h2><ul>
<li>partitioner优化，使用mapValues代替map，保留分区信息。<ul>
<li>RDD中使用partitionBy方法方法分区，然后在尽量不破坏分区的情况下（如：不使用map），可以获得性能收益。典型的情况是<strong>需要多次join，且数据倾斜</strong>。对数据量很大的RDD执行HashPartition操作，之后join会不会再发送大量数据移动，只会移动小的表到已经Partition好的RDD上。</li>
<li>DataFrame中的应用，参考这个<a href="http://stackoverflow.com/questions/30995699/how-to-define-partitioning-of-dataframe" target="_blank" rel="noopener">stackoverflow提问</a>。一般情况下，不需要使用这个方法，因为底层<a href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html" target="_blank" rel="noopener">Catalyst Optimizer</a>会优化。<ul>
<li>1.6及其以上版本：<code>df.repartition($&quot;key&quot;)</code>对某列使用内置的Partition进行<strong>Hash分区</strong>。这是<strong>推荐的方法</strong>。</li>
<li>小于1.6：先对RDD进行partition，再构造df。会保留改RDD的partition方式。</li>
</ul>
</li>
</ul>
</li>
<li>使用广播变量，发送较大的table：应用在依据一个table来过滤数据。</li>
<li>使用基于分区的操作mapPartition代替map，减少数据库连接、网络连接数目（相对于一条一条处理数据）。此外还有mapPartitionsWithIndex,foreachPartitions</li>
<li>选择合适的序列化器：<a href="http://spark.apache.org/docs/latest/tuning.html#data-serialization" target="_blank" rel="noopener">参考文章</a></li>
<li>内存优化：<a href="http://spark.apache.org/docs/latest/tuning.html#memory-tuning" target="_blank" rel="noopener">参考文章</a></li>
<li>了解调度的方式：应用间调度，应用内调度。<a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">参考文章</a></li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至此，简单地介绍了Spark的配置和常用API，用来解决遇到的常见问题。其中<strong>配置部分</strong>我们很少关心，毕竟在业务开发中我们往往使用公司配置好了的Spark环境，但是调优和解决问题往往能从中入手，是理解Spark的重要一环，这个我会在了解Spark的内部原理之后再详细说明配置项的含义。<strong>API编程</strong>层面主要是RDD与DataSet/DataFrame的操作，熟悉常用的function工具与一下场景实践，同时介绍了<strong>Shuffle,Partition和Cache的概念</strong>。最后简单提了一下<strong>Spark Streaming</strong>和<strong>性能优化</strong>。后期，<strong>工程方面</strong>我会在这几个方面深入研究:</p>
<ul>
<li>Spark的<strong>基本原理与源代码</strong>，在深层次上理解Spark。</li>
<li>解决流式问题的<strong>Spark Streaming</strong>深入探究，包括且不限于流式的常见问题与Spark上的策略。</li>
<li>常见的问题：<strong>性能调优</strong>与Spark如何优雅的<strong>工程化</strong>，涉及内存调优，Scala函数式编程等。</li>
<li>Spark社区新特性与发展方向。</li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

            <div class="social-item">
              <a target="_blank" class="social-link" href="https://t.me/mltalk">
                <span class="icon">
                  <i class="fa fa-telegram"></i>
                </span>

                <span class="label">机器学习碎碎念</span>
              </a>
            </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/大数据分析/" rel="tag"># 大数据分析</a>
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/01/02/致2016，你好2017/" rel="prev" title="致2016，你好2017">
      <i class="fa fa-chevron-left"></i> 致2016，你好2017
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/04/09/流式超越批量-Streaming 102翻译/" rel="next" title="流式超越批量：Streaming 102翻译">
      流式超越批量：Streaming 102翻译 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-number">2.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#开发环境配置"><span class="nav-number">3.</span> <span class="nav-text">开发环境配置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#配置"><span class="nav-number">3.1.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动"><span class="nav-number">3.2.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置的介绍"><span class="nav-number">3.3.</span> <span class="nav-text">配置的介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基本API"><span class="nav-number">4.</span> <span class="nav-text">基本API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD"><span class="nav-number">4.1.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建RDD"><span class="nav-number">4.1.1.</span> <span class="nav-text">创建RDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD的分类（重点）"><span class="nav-number">4.1.2.</span> <span class="nav-text">RDD的分类（重点）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核心API"><span class="nav-number">4.1.3.</span> <span class="nav-text">核心API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核心概念（重点）"><span class="nav-number">4.1.4.</span> <span class="nav-text">核心概念（重点）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#注意点"><span class="nav-number">4.1.5.</span> <span class="nav-text">注意点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-O方式"><span class="nav-number">4.2.</span> <span class="nav-text">I/O方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共享变量"><span class="nav-number">4.3.</span> <span class="nav-text">共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#累加器"><span class="nav-number">4.3.1.</span> <span class="nav-text">累加器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广播变量"><span class="nav-number">4.3.2.</span> <span class="nav-text">广播变量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">5.</span> <span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#理解RDD-DataFrame-DataSet几种API的本质区别"><span class="nav-number">5.1.</span> <span class="nav-text">理解RDD DataFrame DataSet几种API的本质区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核心API-1"><span class="nav-number">5.2.</span> <span class="nav-text">核心API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-O方式-1"><span class="nav-number">5.3.</span> <span class="nav-text">I/O方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见需求最佳实践"><span class="nav-number">5.4.</span> <span class="nav-text">常见需求最佳实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">6.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他"><span class="nav-number">7.</span> <span class="nav-text">其他</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#性能优化的基本技巧"><span class="nav-number">8.</span> <span class="nav-text">性能优化的基本技巧</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">9.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">limuzhi</p>
  <div class="site-description" itemprop="description">something about tech, android etc...</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/muzhi1991" title="GitHub → https://github.com/muzhi1991" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:muzhi1991@gmail.com" title="E-Mail → mailto:muzhi1991@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/muzhi1991" title="Twitter → https://twitter.com/muzhi1991" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.chenyupeng.com/" title="https://www.chenyupeng.com/" rel="noopener" target="_blank">陈玉鹏的个人空间</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://macshuo.com/" title="http://macshuo.com/" rel="noopener" target="_blank">MacTalk</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">limuzhi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'G5HLDFmPsllxIjax4F2JTLnl-gzGzoHsz',
      appKey     : 'A5PTgbvpJwjPlcBJ3Brl8rDs',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
